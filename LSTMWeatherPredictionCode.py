# -*- coding: utf-8 -*-
"""BigDataProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GQqALX-dBs9veo6bkpaPyk6e9iXTXCng

**Applying Recurrent Neural Networks for Weather feature predictions**
"""

import numpy as np
import pandas as pd
import keras
from keras.models import Sequential
from keras.layers import LSTM,Dense ,Dropout
import sklearn
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt

from google.colab import files
data_to_load = files.upload()

"""# ***TEMPERATURE***

---
"""

import io
df = pd.read_csv(io.BytesIO(data_to_load['temperature.csv']))

display(df)

#keeping only the temperature column for each city and removing null values

# temperature of san antonio
tempS =  df.drop(df.iloc[:,0:11].columns, axis = 1); tempS = tempS.drop(tempS.iloc[:,1:].columns,axis =1)
tempS = tempS.rename(columns={tempS.columns[0]: " San Antonio temperature(K)"})
tempS = tempS.dropna(axis=0); tempS= tempS.reset_index(drop=True)

# temperature of dallas
tempD =  df.drop(df.iloc[:,0:12].columns, axis = 1); tempD = tempD.drop(tempD.iloc[:,1:].columns,axis =1)
tempD = tempD.rename(columns={tempD.columns[0]: " Dallas temperature(K)"})
tempD = tempD.dropna(axis=0); tempD= tempD.reset_index(drop=True)

# temperature of houston
tempH =  df.drop(df.iloc[:,0:13].columns, axis = 1); tempH = tempH.drop(tempH.iloc[:,1:].columns,axis =1)
tempH = tempH.rename(columns={tempH.columns[0]: " Houston temperature(K)"})
tempH = tempH.dropna(axis=0); tempH= tempH.reset_index(drop=True)

len(tempD)

"""**MODELING FOR SAN ANTONIO CITY - TEMPERATURE**"""

# Modeling for San Antonio Temperature
training_data = tempS.iloc[:30000]; testing_data = tempS.iloc[30000:]

#Normalizing the data
scaler_sa = MinMaxScaler(feature_range=(0,1)); training_data_normalized = scaler_sa.fit_transform(training_data)

# Split into input and outputs for San Antonio temperature
x_train_sa = []; y_train_sa = []
next = 4; past= 30

for i in range(0,len(training_data_normalized)-past-next+1):
    x_train_sa.append(training_data_normalized[i : i + past , 0])     
    y_train_sa.append(training_data_normalized[i + past : i + past + next , 0 ])

x_train_sa , y_train_sa = np.array(x_train_sa), np.array(y_train_sa)

#x_train_sa = np.reshape(x_train_sa, (x_train_sa.shape[0] , 1, x_train_sa.shape[1]) )
x_train_sa = np.reshape(x_train_sa, (x_train_sa.shape[0] , x_train_sa.shape[1],1) )

x_train_sa.shape

# Training for 25 Epochs
model_saTemp = Sequential()

model_saTemp.add(LSTM(units=30, input_shape = (x_train_sa.shape[1],1))); model_saTemp.add(Dense(units = next,activation='softmax'))
model_saTemp.compile(optimizer='adam', loss='mean_squared_error',metrics=['acc'])

history_sa = model_saTemp.fit(x_train_sa, y_train_sa, epochs=25,batch_size=100 )

#Plot testing accuracy vs Epochs
plt.plot(history_sa.history['acc']); plt.title('SAN ANTONIO: Training accuracy vs Epochs, alpha = 0.01')
plt.ylabel('Accuracy'); plt.xlabel('Epochs')
plt.show()

#Plot training loss vs Epochs
plt.plot(history_sa.history['loss']); plt.title('SAN ANTONIO: Training Loss vs Epochs, alpha = 0.01')
plt.ylabel('Loss'); plt.xlabel('Epochs')
plt.show()

print(f"Training accuracy 25 epochs(lr 0.001): {history_sa.history['acc'][-1] * 100} " ); print(f"Training loss 25 epochs(lr 0.001): {history_sa.history['loss'][-1]} " )

print(history_sa.epoch)

"""**SECOND MODEL FOR SAN ANTONIO - TEMPERATURE (Slightly improves accuracy compared to first model above)**"""

model2_saTemp = Sequential()

model2_saTemp.add(LSTM(units=50,return_sequences=True, input_shape = (x_train_sa.shape[1],x_train_sa.shape[2])))
model2_saTemp.add(Dropout(0.2))
model2_saTemp.add(LSTM(32, return_sequences=True))
model2_saTemp.add(Dropout(0.2))
model2_saTemp.add(LSTM(32))
model2_saTemp.add(Dense(units = next,activation='softmax'))

opt = keras.optimizers.Adam(learning_rate=0.001)
model2_saTemp.compile(optimizer=opt, loss='mean_squared_error',metrics=['acc'])

history_sa2 = model2_saTemp.fit(x_train_sa, y_train_sa, epochs=50,batch_size=32)

#Plot testing accuracy vs Epochs
plt.plot(history_sa2.history['acc']); plt.title('SAN ANTONIO: Training accuracy Vs Epochs, alpha = 0.001'); plt.ylabel('Accuracy'); plt.xlabel('Epochs')
plt.show()

#Plot training loss vs Epochs
plt.plot(history_sa2.history['loss']); plt.title('SAN ANTONIO: Training Loss vs Epochs, alpha = 0.001'); plt.ylabel('Loss'); plt.xlabel('Epochs')
plt.show()

print(f"Training accuracy 25 epochs(lr 0.001): {history_sa2.history['acc'][-1] * 100} " ); print(f"Training loss 25 epochs(lr 0.001): {history_sa2.history['loss'][-1]} " )

print(history_sa2.epoch)

"""**MODELING FOR DALLAS CITY - TEMPERATURE**"""

# Modeling for Dallas Temperature
training_data = tempD.iloc[:30000]; testing_data = tempD.iloc[30000:]

#Normalizing the data
scaler_da = MinMaxScaler(feature_range=(0,1)); training_data_normalized = scaler_da.fit_transform(training_data)

# Split into input and outputs for Dallas temperature
x_train_da = []; y_train_da = []
next = 4; past= 30

for i in range(0,len(training_data_normalized)-past-next+1):
    x_train_da.append(training_data_normalized[i : i + past , 0])     
    y_train_da.append(training_data_normalized[i + past : i + past + next , 0 ])

x_train_da , y_train_da = np.array(x_train_da), np.array(y_train_da)

#x_train_da = np.reshape(x_train_da, (x_train_da.shape[0] , 1, x_train_da.shape[1]) )
x_train_da = np.reshape(x_train_da, (x_train_da.shape[0] , x_train_da.shape[1],1) )

# Training for 25 Epochs
model_daTemp = Sequential()

model_daTemp.add(LSTM(units=30, input_shape = (x_train_da.shape[1],1))); model_daTemp.add(Dense(units = next,activation='softmax'))
model_daTemp.compile(optimizer='adam', loss='mean_squared_error',metrics=['acc'])

history_da = model_daTemp.fit(x_train_da, y_train_da, epochs=25,batch_size=100 )

#Plot testing accuracy vs Epochs
plt.plot(history_da.history['acc']); plt.title('DALLAS: Training accuracy vs Epochs, alpha = 0.01'); plt.ylabel('Accuracy'); plt.xlabel('Epochs')
plt.show()

#Plot training loss vs Epochs
plt.plot(history_da.history['loss']); plt.title('DALLAS: Training Loss vs Epochs, alpha = 0.01'); plt.ylabel('Loss'); plt.xlabel('Epochs')
plt.show()

print(f"Training accuracy 25 epochs(lr 0.01): {history_da.history['acc'][-1] * 100} "); print(f"Training loss 25 epochs(lr 0.01): {history_da.history['loss'][-1]} " )

print(history_da.epoch)

"""**SECOND MODEL FOR DALLAS - TEMPERATURE (With slightly improved accuracy)**"""

model2_daTemp = Sequential()

model2_daTemp.add(LSTM(units=50,return_sequences=True, input_shape = (x_train_da.shape[1],1)))
model2_daTemp.add(Dropout(0.2))
model2_daTemp.add(LSTM(32, return_sequences=True))
model2_daTemp.add(Dropout(0.2))
model2_daTemp.add(LSTM(32))
model2_daTemp.add(Dense(units = next,activation='softmax'))

opt = keras.optimizers.Adam(learning_rate=0.001)
model2_daTemp.compile(optimizer=opt, loss='mean_squared_error',metrics=['acc'])

history_da2 = model2_daTemp.fit(x_train_da, y_train_da, epochs=50,batch_size=32 )

#Plot testing accuracy vs Epochs
plt.plot(history_da2.history['acc']); plt.title('DALLAS: Training accuracy vs Epochs, alpha = 0.001'); plt.ylabel('Accuracy'); plt.xlabel('Epochs')
plt.show()

#Plot training loss vs Epochs
plt.plot(history_da2.history['loss']); plt.title('DALLAS: Training Loss vs Epochs, alpha = 0.001'); plt.ylabel('Loss'); plt.xlabel('Epochs')
plt.show()

print(f"Training accuracy 25 epochs(lr 0.001): {history_da2.history['acc'][-1] * 100} "); print(f"Training loss 25 epochs(lr 0.001): {history_da2.history['loss'][-1]} " )

print(history_da2.epoch)

"""**MODELING FOR HOUSTON CITY - TEMPERATURE**"""

# Modeling for Houston Temperature
training_data = tempH.iloc[:30000]; testing_data = tempH.iloc[30000:]

#Normalizing the data
scaler_hou = MinMaxScaler(feature_range=(0,1)); training_data_normalized = scaler_hou.fit_transform(training_data)

# Split into input and outputs for Houston temperature
x_train_hou = []; y_train_hou = []
next = 4; past= 30

for i in range(0,len(training_data_normalized)-past-next+1):
    x_train_hou.append(training_data_normalized[i : i + past , 0])     
    y_train_hou.append(training_data_normalized[i + past : i + past + next , 0 ])

x_train_hou , y_train_hou = np.array(x_train_hou), np.array(y_train_hou)

#x_train_hou = np.reshape(x_train_hou, (x_train_hou.shape[0] , 1, x_train_hou.shape[1]) )
x_train_hou = np.reshape(x_train_hou, (x_train_hou.shape[0] , x_train_hou.shape[1],1) )

# Training for 25 Epochs
model_houTemp = Sequential()

model_houTemp.add(LSTM(units=30, input_shape = (x_train_hou.shape[1],1))); model_houTemp.add(Dense(units = next,activation='softmax'))
model_houTemp.compile(optimizer='adam', loss='mean_squared_error',metrics=['acc'])

history_hou = model_houTemp.fit(x_train_hou, y_train_hou, epochs=25,batch_size=100 )

#Plot testing accuracy vs Epochs
plt.plot(history_hou.history['acc']); plt.title('HOUSTON: Training accuracy Vs Epochs, alpha = 0.01'); plt.ylabel('Accuracy'); plt.xlabel('Epochs')
plt.show()

#Plot training loss vs Epochs
plt.plot(history_hou.history['loss']); plt.title('HOUSTON: Training Loss vs Epochs, alpha = 0.01'); plt.ylabel('Loss'); plt.xlabel('Epochs')
plt.show()

print(f"Training accuracy 25 epochs(lr 0.001): {history_hou.history['acc'][-1] * 100} "); print(f"Training loss 25 epochs(lr 0.001): {history_hou.history['loss'][-1]} " )

print(history_hou.epoch)

"""**Second model for Houston Temperature**"""

model2_houTemp = Sequential()

model2_houTemp.add(LSTM(units=50,return_sequences=True, input_shape = (x_train_hou.shape[1],1)))
model2_houTemp.add(Dropout(0.2))
model2_houTemp.add(LSTM(32, return_sequences=True))
model2_houTemp.add(Dropout(0.2))
model2_houTemp.add(LSTM(32))
model2_houTemp.add(Dense(units = next,activation='softmax'))

opt = keras.optimizers.Adam(learning_rate=0.001)
model2_houTemp.compile(optimizer=opt, loss='mean_squared_error',metrics=['acc'])

history2_hou = model2_houTemp.fit(x_train_hou, y_train_hou, epochs=50,batch_size=32 )

#Plot testing accuracy vs Epochs
plt.plot(history2_hou.history['acc']); plt.title('HOUSTON: Training accuracy vs Epochs, alpha = 0.001'); plt.ylabel('Accuracy'); plt.xlabel('Epochs')
plt.show()

#Plot training loss vs Epochs
plt.plot(history2_hou.history['loss']); plt.title('HOUSTON: Training Loss vs Epochs, alpha = 0.001'); plt.ylabel('Loss'); plt.xlabel('Epochs')
plt.show()

print(f"Training accuracy 25 epochs(lr 0.001): {history2_hou.history['acc'][-1] * 100} "); print(f"Training loss 25 epochs(lr 0.001): {history2_hou.history['loss'][-1]} " )

print(history2_hou.epoch)

"""# ***HUMIDITY***

---
"""

from google.colab import files
data_to_load = files.upload()

import io
df2 = pd.read_csv(io.BytesIO(data_to_load['humidity.csv']))

display(df2)

#keeping only the humidity column for each city and removing null values

# humidity of san antonio
humS =  df2.drop(df2.iloc[:,0:11].columns, axis = 1); humS = humS.drop(humS.iloc[:,1:].columns,axis =1)
humS = humS.rename(columns={humS.columns[0]: " San Antonio humidity"})
humS = humS.dropna(axis=0) ; humS= humS.reset_index(drop=True)

# humidity of dallas
humD =  df2.drop(df2.iloc[:,0:12].columns, axis = 1); humD = humD.drop(humD.iloc[:,1:].columns,axis =1)
humD = humD.rename(columns={humD.columns[0]: " Dallas humidity"})
humD = humD.dropna(axis=0) ; humD= humD.reset_index(drop=True)

# humidity of houston
humH =  df2.drop(df2.iloc[:,0:13].columns, axis = 1); humH = humH.drop(humH.iloc[:,1:].columns,axis =1)
humH = humH.rename(columns={humH.columns[0]: " Houston humidity"})
humH = humH.dropna(axis=0); humH= humH.reset_index(drop=True)

"""**MODELING FOR SAN ANTONIO CITY - HUMIDITY**"""

# Modeling for San Antonio Humidity
training_data = humS.iloc[:30000]; testing_data = humS.iloc[30000:]

#Normalizing the data
scaler_sa = MinMaxScaler(feature_range=(0,1)); training_data_normalized = scaler_sa.fit_transform(training_data)

# Split into input and outputs for San Antonio humidity
x_train_sa = []; y_train_sa = []
next = 4; past= 30

for i in range(0,len(training_data_normalized)-past-next+1):
    x_train_sa.append(training_data_normalized[i : i + past , 0])     
    y_train_sa.append(training_data_normalized[i + past : i + past + next , 0 ])

x_train_sa , y_train_sa = np.array(x_train_sa), np.array(y_train_sa)

x_train_sa = np.reshape(x_train_sa, (x_train_sa.shape[0] , 1, x_train_sa.shape[1]))

x_train_sa.shape

from keras.layers import LSTM,Dense,Dropout

# Training for 25 Epochs
model_saHum = Sequential()

model_saHum.add(LSTM(units=50, return_sequences=True, input_shape = (x_train_sa.shape[1],x_train_sa.shape[2])))
model_saHum.add(Dropout(0.2))
model_saHum.add(LSTM(32, return_sequences=True))
model_saHum.add(Dropout(0.2))
model_saHum.add(LSTM(32))
model_saHum.add(Dense(units = next,activation='softmax'))
model_saHum.compile(optimizer='adam', loss='mean_squared_error',metrics=['acc'])

history_sa = model_saHum.fit(x_train_sa, y_train_sa, epochs=25, batch_size=100 )

import matplotlib.pyplot as plt
#Plot testing accuracy vs Epochs
plt.plot(history_sa.history['acc']); plt.title('SAN ANTONIO: Training accuracy vs Epochs, alpha = 0.01'); plt.ylabel('Accuracy'); plt.xlabel('Epochs')
plt.show()

#Plot training loss vs Epochs
plt.plot(history_sa.history['loss']); plt.title('SAN ANTONIO: Training Loss vs Epochs, alpha = 0.01'); plt.ylabel('Loss'); plt.xlabel('Epochs')
plt.show()

print(f"Training accuracy 25 epochs(lr 0.01): {history_sa.history['acc'][-1] * 100} "); print(f"Training loss 25 epochs(lr 0.01): {history_sa.history['loss'][-1]} " )

print(history_sa.epoch)

from keras.layers import LSTM,Dense,Dropout

# Training for 45 Epochs with batch size 75
model_saHum = Sequential()

model_saHum.add(LSTM(units=50, return_sequences=True, input_shape = (x_train_sa.shape[1],x_train_sa.shape[2])))
model_saHum.add(Dropout(0.2))
model_saHum.add(LSTM(32, return_sequences=True))
model_saHum.add(Dropout(0.2))
model_saHum.add(LSTM(32))
model_saHum.add(Dense(units = next,activation='softmax'))
model_saHum.compile(optimizer='adam', loss='mean_squared_error',metrics=['acc'])

history_sa = model_saHum.fit(x_train_sa, y_train_sa, epochs=45,batch_size=75 )

import matplotlib.pyplot as plt
#Plot testing accuracy vs Epochs -  Higher epochs, lower batch
plt.plot(history_sa.history['acc']); plt.title('SAN ANTONIO: Training accuracy vs Epochs, alpha = 0.01'); plt.ylabel('Accuracy'); plt.xlabel('Epochs')
plt.show()

#Plot training loss vs Epochs
plt.plot(history_sa.history['loss']); plt.title('SAN ANTONIO: Training Loss vs Epochs, alpha = 0.01'); plt.ylabel('Loss'); plt.xlabel('Epochs')
plt.show()

print(f"Training accuracy 25 epochs(lr 0.01): {history_sa.history['acc'][-1] * 100} "); print(f"Training loss 25 epochs(lr 0.01): {history_sa.history['loss'][-1]} " )

print(history_sa.epoch)

"""**MODELING FOR DALLAS CITY - HUMIDITY**"""

# Modeling for Dallas Humidity
training_data = humD.iloc[:30000]; testing_data = humD.iloc[30000:]

#Normalizing the data
scaler_da = MinMaxScaler(feature_range=(0,1)); training_data_normalized = scaler_da.fit_transform(training_data)

# Split into input and outputs for Dallas humidity
x_train_da = []; y_train_da = []
next = 4; past= 30

for i in range(0,len(training_data_normalized)-past-next+1):
    x_train_da.append(training_data_normalized[i : i + past , 0])     
    y_train_da.append(training_data_normalized[i + past : i + past + next , 0 ])

x_train_da , y_train_da = np.array(x_train_da), np.array(y_train_da)

x_train_da = np.reshape(x_train_da, (x_train_da.shape[0] , 1, x_train_da.shape[1]))

x_train_da.shape

from keras.layers import LSTM,Dense,Dropout

# Training for 25 Epochs
model_daHum = Sequential()

model_daHum.add(LSTM(units=50, return_sequences=True, input_shape = (x_train_da.shape[1],x_train_da.shape[2])))
model_daHum.add(Dropout(0.2))
model_daHum.add(LSTM(32, return_sequences=True))
model_daHum.add(Dropout(0.2))
model_daHum.add(LSTM(32))
model_daHum.add(Dense(units = next,activation='softmax'))
model_daHum.compile(optimizer='adam', loss='mean_squared_error',metrics=['acc'])

history_da = model_daHum.fit(x_train_da, y_train_da, epochs=25,batch_size=100 )

import matplotlib.pyplot as plt
#Plot testing accuracy vs Epochs
plt.plot(history_da.history['acc']); plt.title('DALLAS: Training accuracy vs Epochs, alpha = 0.01'); plt.ylabel('Accuracy'); plt.xlabel('Epochs')
plt.show()

#Plot training loss vs Epochs
plt.plot(history_da.history['loss']); plt.title('DALLAS: Training Loss vs Epochs, alpha = 0.01'); plt.ylabel('Loss'); plt.xlabel('Epochs')
plt.show()

print(f"Training accuracy 25 epochs(lr 0.01): {history_da.history['acc'][-1] * 100} "); print(f"Training loss 25 epochs(lr 0.01): {history_da.history['loss'][-1]} ")

print(history_da.epoch)

from keras.layers import LSTM,Dense,Dropout

# Training for 45 Epochs, batch size = 75
model_daHum = Sequential()

model_daHum.add(LSTM(units=50, return_sequences=True, input_shape = (x_train_da.shape[1],x_train_da.shape[2])))
model_daHum.add(Dropout(0.2))
model_daHum.add(LSTM(32, return_sequences=True))
model_daHum.add(Dropout(0.2))
model_daHum.add(LSTM(32))
model_daHum.add(Dense(units = next,activation='softmax'))
model_daHum.compile(optimizer='adam', loss='mean_squared_error',metrics=['acc'])

history_da = model_daHum.fit(x_train_da, y_train_da, epochs=45, batch_size=75 )

import matplotlib.pyplot as plt
#Plot testing accuracy vs Epochs - higher epochs, higher batch
plt.plot(history_da.history['acc']); plt.title('DALLAS: Training accuracy vs Epochs, alpha = 0.01'); plt.ylabel('Accuracy'); plt.xlabel('Epochs')
plt.show()

#Plot training loss vs Epochs
plt.plot(history_da.history['loss']); plt.title('DALLAS: Training Loss vs Epochs, alpha = 0.01'); plt.ylabel('Loss'); plt.xlabel('Epochs')
plt.show()

print(f"Training accuracy 25 epochs(lr 0.01): {history_da.history['acc'][-1] * 100} "); print(f"Training loss 25 epochs(lr 0.01): {history_da.history['loss'][-1]} ")

print(history_da.epoch)

"""**MODELING FOR HOUSTON CITY - HUMIDITY**"""

# Modeling for Houston Humidity
training_data = humH.iloc[:30000]; testing_data = humH.iloc[30000:]

#Normalizing the data
scaler_ha = MinMaxScaler(feature_range=(0,1)); training_data_normalized = scaler_ha.fit_transform(training_data)

# Split into input and outputs for Houston humidity
x_train_ha = []; y_train_ha = []
next = 4; past= 30

for i in range(0,len(training_data_normalized)-past-next+1):
    x_train_ha.append(training_data_normalized[i : i + past , 0])     
    y_train_ha.append(training_data_normalized[i + past : i + past + next , 0 ])

x_train_ha , y_train_ha = np.array(x_train_ha), np.array(y_train_ha)

x_train_ha = np.reshape(x_train_ha, (x_train_ha.shape[0] , 1, x_train_ha.shape[1]))

x_train_ha.shape

from keras.layers import LSTM,Dense,Dropout

# Training for 25 Epochs
model_haHum = Sequential()

model_haHum.add(LSTM(units=50, return_sequences=True, input_shape = (x_train_ha.shape[1],x_train_ha.shape[2])))
model_haHum.add(Dropout(0.2))
model_haHum.add(LSTM(32, return_sequences=True))
model_haHum.add(Dropout(0.2))
model_haHum.add(LSTM(32))
model_haHum.add(Dense(units = next,activation='softmax'))
model_haHum.compile(optimizer='adam', loss='mean_squared_error',metrics=['acc'])

history_ha = model_haHum.fit(x_train_ha, y_train_ha, epochs=25,batch_size=100 )

import matplotlib.pyplot as plt
#Plot testing accuracy vs Epochs
plt.plot(history_ha.history['acc']); plt.title('HOUSTON: Training accuracy vs Epochs, alpha = 0.01'); plt.ylabel('Accuracy'); plt.xlabel('Epochs')
plt.show()

#Plot training loss vs Epochs
plt.plot(history_ha.history['loss']); plt.title('HOUSTON: Training Loss vs Epochs, alpha = 0.01'); plt.ylabel('Loss'); plt.xlabel('Epochs')
plt.show()

print(f"Training accuracy 25 epochs(lr 0.01): {history_ha.history['acc'][-1] * 100} "); print(f"Training loss 25 epochs(lr 0.01): {history_ha.history['loss'][-1]} ")

print(history_ha.epoch)

from keras.layers import LSTM,Dense,Dropout

# Training for 45 Epochs, batch size= 75
model_haHum = Sequential()

model_haHum.add(LSTM(units=50, return_sequences=True, input_shape = (x_train_ha.shape[1],x_train_ha.shape[2])))
model_haHum.add(Dropout(0.2))
model_haHum.add(LSTM(32, return_sequences=True))
model_haHum.add(Dropout(0.2))
model_haHum.add(LSTM(32))
model_haHum.add(Dense(units = next,activation='softmax'))
model_haHum.compile(optimizer='adam', loss='mean_squared_error',metrics=['acc'])

history_ha = model_haHum.fit(x_train_ha, y_train_ha, epochs=45, batch_size=75 )

import matplotlib.pyplot as plt
#Plot testing accuracy vs Epochs -  higher epochs, higher batch
plt.plot(history_ha.history['acc']); plt.title('HOUSTON: Training accuracy vs Epochs, alpha = 0.01'); plt.ylabel('Accuracy'); plt.xlabel('Epochs')
plt.show()

#Plot training loss vs Epochs
plt.plot(history_ha.history['loss']); plt.title('HOUSTON: Training Loss vs Epochs, alpha = 0.01'); plt.ylabel('Loss'); plt.xlabel('Epochs')
plt.show()

print(f"Training accuracy 25 epochs(lr 0.01): {history_ha.history['acc'][-1] * 100} "); print(f"Training loss 25 epochs(lr 0.01): {history_ha.history['loss'][-1]} ")

print(history_ha.epoch)